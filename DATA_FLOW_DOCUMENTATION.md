# ğŸ“Š CryptoPredict Data Flow Documentation

## Overview
Complete data flow from raw cryptocurrency data to predictions, including all files, classes, and methods.

---

## ğŸ¯ Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    TRAINING PIPELINE                         â”‚
â”‚  (train.py â†’ 7 Pipeline Stages â†’ Model Artifacts)          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   PREDICTION PIPELINE                        â”‚
â”‚  (app.py â†’ API Data â†’ Feature Engineering â†’ Models)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ Project Structure

```
VinoPredict/
â”œâ”€â”€ train.py                          # Training entry point
â”œâ”€â”€ app.py                            # Flask web application
â”œâ”€â”€ config/
â”‚   â””â”€â”€ config.yaml                   # Configuration
â”œâ”€â”€ src/mlProject/
â”‚   â”œâ”€â”€ components/                   # Core components
â”‚   â”‚   â”œâ”€â”€ crypto_data_ingestion.py
â”‚   â”‚   â”œâ”€â”€ data_validation.py
â”‚   â”‚   â”œâ”€â”€ data_transformation.py
â”‚   â”‚   â”œâ”€â”€ model_trainer.py
â”‚   â”‚   â”œâ”€â”€ model_evaluation.py
â”‚   â”‚   â”œâ”€â”€ deep_model_trainer.py
â”‚   â”‚   â”œâ”€â”€ deep_model_evaluation.py
â”‚   â”‚   â”œâ”€â”€ feature_engineering.py    # NEW: Shared features
â”‚   â”‚   â””â”€â”€ feature_validator.py      # NEW: Validation
â”‚   â”œâ”€â”€ pipeline/                     # Pipeline orchestration
â”‚   â”‚   â”œâ”€â”€ stage_01_data_ingestion.py
â”‚   â”‚   â”œâ”€â”€ stage_02_data_validation.py
â”‚   â”‚   â”œâ”€â”€ stage_03_data_transformation.py
â”‚   â”‚   â”œâ”€â”€ stage_04_model_trainer.py
â”‚   â”‚   â”œâ”€â”€ stage_05_model_evaluation.py
â”‚   â”‚   â”œâ”€â”€ stage_06_deep_model_trainer.py
â”‚   â”‚   â”œâ”€â”€ stage_07_deep_model_evaluation.py
â”‚   â”‚   â””â”€â”€ unified_prediction.py     # Prediction pipeline
â”‚   â”œâ”€â”€ config/
â”‚   â”‚   â””â”€â”€ configuration.py          # Config manager
â”‚   â””â”€â”€ entity/
â”‚       â””â”€â”€ config_entity.py          # Config dataclasses
â””â”€â”€ artifacts/                        # Generated artifacts
    â”œâ”€â”€ data_ingestion/
    â”œâ”€â”€ data_transformation/
    â”œâ”€â”€ model_trainer/
    â”œâ”€â”€ deep_model_trainer/
    â””â”€â”€ feature_metadata.json         # NEW: Feature stats
```

---

## ğŸš‚ TRAINING PIPELINE FLOW

### Entry Point: `train.py`

#### Main Function
```python
def main()
    â”œâ”€â”€ parse_arguments()
    â”‚   â””â”€â”€ --model: traditional/deep/both
    â”‚   â””â”€â”€ --skip-data: skip data pipeline
    â”œâ”€â”€ run_data_pipeline()           # Step 1-3
    â”œâ”€â”€ train_traditional_model()     # Step 4-5
    â””â”€â”€ train_deep_learning_model()   # Step 6-7
```

---

### STAGE 1: Data Ingestion

**File**: `src/mlProject/pipeline/stage_01_data_ingestion.py`

**Class**: `DataIngestionTrainingPipeline`
```python
def main():
    â”œâ”€â”€ ConfigurationManager().get_data_ingestion_config()
    â”œâ”€â”€ CryptoDataIngestion(config)
    â”œâ”€â”€ download_file(cryptocurrencies)
    â””â”€â”€ extract_zip_file()
```

**Component**: `src/mlProject/components/crypto_data_ingestion.py`

**Class**: `CryptoDataIngestion`
```python
Methods:
â”œâ”€â”€ __init__(config)
â”œâ”€â”€ get_cryptocompare_data(crypto_symbol, limit=2000)
â”‚   â””â”€â”€ Fetches hourly data from CryptoCompare API
â”œâ”€â”€ get_binance_data(symbol, interval='1h', limit=1000)
â”‚   â””â”€â”€ Fetches candlestick data from Binance API
â”œâ”€â”€ get_crypto_data(crypto_id, vs_currency, days=365)
â”‚   â””â”€â”€ Fetches historical data from CoinGecko API
â”œâ”€â”€ add_technical_indicators(df)
â”‚   â”œâ”€â”€ SMA: 7, 14, 30 period moving averages
â”‚   â”œâ”€â”€ EMA: 7, 14 period exponential moving averages
â”‚   â”œâ”€â”€ MACD: macd, signal, histogram
â”‚   â”œâ”€â”€ RSI: 14 period relative strength index
â”‚   â”œâ”€â”€ Bollinger Bands: upper, middle, lower
â”‚   â”œâ”€â”€ Price Changes: 1h, 24h, 7d
â”‚   â”œâ”€â”€ Volume: sma, ratio
â”‚   â””â”€â”€ Volatility & Price Position
â”œâ”€â”€ create_prediction_targets(df)
â”‚   â”œâ”€â”€ target_price_1h: Next hour price
â”‚   â”œâ”€â”€ target_price_24h: Next day price
â”‚   â”œâ”€â”€ target_direction_1h: Up/Down (0/1)
â”‚   â”œâ”€â”€ target_direction_24h: Up/Down (0/1)
â”‚   â”œâ”€â”€ target_change_1h: Percentage change
â”‚   â””â”€â”€ target_change_24h: Percentage change
â””â”€â”€ download_file(cryptocurrencies)
    â”œâ”€â”€ Aggregates data from 3 APIs
    â”œâ”€â”€ Combines & deduplicates
    â”œâ”€â”€ Adds technical indicators
    â”œâ”€â”€ Creates prediction targets
    â””â”€â”€ Saves to: artifacts/data_ingestion/crypto_data.csv
```

**Output**: 
- CSV with 30 columns (29 features + 1 crypto_symbol)
- Columns: price, volume, market_cap, sma_7, sma_14, sma_30, ema_7, ema_14, 
  macd, macd_signal, macd_histogram, rsi, bb_middle, bb_upper, bb_lower,
  price_change_1h, price_change_24h, price_change_7d, volume_sma, volume_ratio,
  volatility, high_14d, low_14d, price_position, target_price_1h, 
  target_price_24h, target_direction_1h, target_direction_24h,
  target_change_1h, target_change_24h, crypto_symbol

---

### STAGE 2: Data Validation

**File**: `src/mlProject/pipeline/stage_02_data_validation.py`

**Class**: `DataValidationTrainingPipeline`
```python
def main():
    â”œâ”€â”€ ConfigurationManager().get_data_validation_config()
    â”œâ”€â”€ DataValidation(config)
    â””â”€â”€ validate_all_columns()
```

**Component**: `src/mlProject/components/data_validation.py`

**Class**: `DataValidation`
```python
Methods:
â”œâ”€â”€ __init__(config)
â””â”€â”€ validate_all_columns()
    â”œâ”€â”€ Reads CSV from data_ingestion
    â”œâ”€â”€ Checks all required columns exist
    â”œâ”€â”€ Validates against schema.yaml
    â””â”€â”€ Writes status to: artifacts/data_validation/status.txt
```

**Output**: Status file (Validation status: True/False)

---

### STAGE 3: Data Transformation

**File**: `src/mlProject/pipeline/stage_03_data_transformation.py`

**Class**: `DataTransformationTrainingPipeline`
```python
def main():
    â”œâ”€â”€ ConfigurationManager().get_data_transformation_config()
    â”œâ”€â”€ DataTransformation(config)
    â””â”€â”€ train_test_spliting()
```

**Component**: `src/mlProject/components/data_transformation.py`

**Class**: `DataTransformation`
```python
Methods:
â”œâ”€â”€ __init__(config)
â””â”€â”€ train_test_spliting()
    â”œâ”€â”€ Reads: artifacts/data_ingestion/crypto_data.csv
    â”œâ”€â”€ Drops: crypto_symbol column
    â”œâ”€â”€ train_test_split(test_size=0.25)
    â”œâ”€â”€ Saves: artifacts/data_transformation/train.csv
    â””â”€â”€ Saves: artifacts/data_transformation/test.csv
```

**Output**: 
- train.csv: 75% of data (30 columns including target_price_1h)
- test.csv: 25% of data

---

### STAGE 4: Traditional Model Training

**File**: `src/mlProject/pipeline/stage_04_model_trainer.py`

**Class**: `ModelTrainerTrainingPipeline`
```python
def main():
    â”œâ”€â”€ ConfigurationManager().get_model_trainer_config()
    â”œâ”€â”€ ModelTrainer(config)
    â””â”€â”€ train()
```

**Component**: `src/mlProject/components/model_trainer.py`

**Class**: `ModelTrainer`
```python
Methods:
â”œâ”€â”€ __init__(config)
â””â”€â”€ train()
    â”œâ”€â”€ Reads: train.csv, test.csv
    â”œâ”€â”€ Drops: target_price_1h column (this is the target)
    â”œâ”€â”€ Creates: train_x (29 features), train_y (target)
    â”œâ”€â”€ Calculates: Feature statistics (mean, std, min, max)
    â”œâ”€â”€ Saves: artifacts/feature_metadata.json (NEW)
    â”œâ”€â”€ Trains: RandomForestRegressor(n_estimators=100, max_depth=20, min_samples_split=5)
    â””â”€â”€ Saves: artifacts/model_trainer/model.joblib

**Output**: 
- model.joblib: Trained Random Forest model
```

**Output**: 
- model.joblib: Trained ElasticNet model
- feature_metadata.json: Statistics for all 29 features

**Features Used** (29 total after dropping target_price_1h):
```
price, volume, market_cap, sma_7, sma_14, sma_30, ema_7, ema_14,
macd, macd_signal, macd_histogram, rsi, bb_middle, bb_upper, bb_lower,
price_change_1h, price_change_24h, price_change_7d, volume_sma, volume_ratio,
volatility, high_14d, low_14d, price_position, target_price_24h,
target_direction_1h, target_direction_24h, target_change_1h, target_change_24h
```

---

### STAGE 5: Traditional Model Evaluation

**File**: `src/mlProject/pipeline/stage_05_model_evaluation.py`

**Class**: `ModelEvaluationTrainingPipeline`
```python
def main():
    â”œâ”€â”€ ConfigurationManager().get_model_evaluation_config()
    â”œâ”€â”€ ModelEvaluation(config)
    â””â”€â”€ log_into_mlflow()
```

**Component**: `src/mlProject/components/model_evaluation.py`

**Class**: `ModelEvaluation`
```python
Methods:
â”œâ”€â”€ __init__(config)
â”œâ”€â”€ eval_metrics(actual, pred)
â”‚   â”œâ”€â”€ RMSE: Root Mean Squared Error
â”‚   â”œâ”€â”€ MAE: Mean Absolute Error
â”‚   â””â”€â”€ RÂ²: R-squared score
â””â”€â”€ log_into_mlflow()
    â”œâ”€â”€ Loads: model.joblib
    â”œâ”€â”€ Evaluates on test set
    â”œâ”€â”€ Logs metrics to MLflow
    â”œâ”€â”€ Logs model to MLflow
    â””â”€â”€ Saves: artifacts/model_evaluation/metrics.json
```

**Output**: 
- metrics.json: {"rmse": X, "mae": Y, "r2": Z}
- MLflow logged: model + metrics

---

### STAGE 6: Deep Learning Model Training

**File**: `src/mlProject/pipeline/stage_06_deep_model_trainer.py`

**Class**: `DeepModelTrainingPipeline`
```python
def main():
    â”œâ”€â”€ ConfigurationManager().get_deep_model_trainer_config()
    â”œâ”€â”€ DeepModelTrainer(config)
    â””â”€â”€ train()
```

**Component**: `src/mlProject/components/deep_model_trainer.py`

**Class**: `DeepModelTrainer` (Placeholder)
```python
Methods:
â”œâ”€â”€ __init__(config)
â””â”€â”€ train()
    â””â”€â”€ Logs: "Using pre-trained PyTorch models"
```

**Note**: Actual training done externally. Models already exist:
- artifacts/deep_model_trainer/best_deep_model.pth
- artifacts/deep_model_trainer/scaler.joblib
- artifacts/deep_model_trainer/model_config.json

---

### STAGE 7: Deep Learning Model Evaluation

**File**: `src/mlProject/pipeline/stage_07_deep_model_evaluation.py`

**Class**: `DeepModelEvaluationPipeline`
```python
def main():
    â”œâ”€â”€ ConfigurationManager().get_deep_model_evaluation_config()
    â”œâ”€â”€ DeepModelEvaluation(config)
    â””â”€â”€ log_into_mlflow()
```

**Component**: `src/mlProject/components/deep_model_evaluation.py`

**Class**: `DeepModelEvaluation`
```python
Methods:
â”œâ”€â”€ __init__(config)
â”œâ”€â”€ CryptoPriceNet(nn.Module)
â”‚   â”œâ”€â”€ Architecture: [29] â†’ [128] â†’ [64] â†’ [32] â†’ [1]
â”‚   â”œâ”€â”€ Activation: ReLU
â”‚   â””â”€â”€ Dropout: 0.2
â””â”€â”€ log_into_mlflow()
    â”œâ”€â”€ Loads: best_deep_model.pth, scaler.joblib
    â”œâ”€â”€ Evaluates on scaled test set
    â”œâ”€â”€ Calculates: RMSE, MAE, RÂ²
    â”œâ”€â”€ Logs to MLflow
    â””â”€â”€ Saves: artifacts/deep_model_evaluation/metrics.json
```

**Output**: 
- metrics.json: {"rmse": X, "mae": Y, "r2": Z}
- MLflow logged: PyTorch model + metrics

---

## ğŸ”® PREDICTION PIPELINE FLOW

### Entry Point: `app.py`

**Main Components**:
```python
Initialization:
â”œâ”€â”€ Flask app
â”œâ”€â”€ FeatureEngineering() - Shared component
â”œâ”€â”€ ConfigurationManager()
â””â”€â”€ Constants (cache, rate limits, etc.)

Routes:
â”œâ”€â”€ GET  /                          â†’ index.html
â”œâ”€â”€ GET  /historical/<crypto_id>   â†’ Historical data
â””â”€â”€ GET  /get_price_predictions/<crypto_id>  â†’ Predictions
```

---

### PREDICTION FLOW (Route: `/get_price_predictions/<crypto_id>`)

```python
@app.route('/get_price_predictions/<crypto_id>')
def get_price_predictions(crypto_id):
    Step 1: Validate cryptocurrency
    â”œâ”€â”€ ConfigurationManager().get_cryptocurrencies()
    â””â”€â”€ Check if crypto_id is supported
    
    Step 2: Fetch live market data
    â”œâ”€â”€ get_cached_api_data()
    â”‚   â”œâ”€â”€ URL: api.coingecko.com/api/v3/simple/price
    â”‚   â”œâ”€â”€ Params: ids, vs_currencies, include_24hr_change, etc.
    â”‚   â”œâ”€â”€ Caching: 5 minute cache
    â”‚   â””â”€â”€ Rate limiting: 1.2s between calls
    â””â”€â”€ Extract: current_price, volume, market_cap
    
    Step 3: Fetch historical data
    â”œâ”€â”€ get_cached_api_data()
    â”‚   â”œâ”€â”€ URL: api.coingecko.com/api/v3/coins/{crypto_id}/market_chart
    â”‚   â”œâ”€â”€ Params: vs_currency=usd, days=2
    â”‚   â””â”€â”€ Returns: 48 hours of price data
    â””â”€â”€ _extract_price_data()
        â””â”€â”€ Extracts last 24 prices for indicators
    
    Step 4: Feature Engineering (NEW - Unified Component)
    â”œâ”€â”€ feature_engineer.prepare_features()
    â”‚   â”œâ”€â”€ current_price
    â”‚   â”œâ”€â”€ current_volume
    â”‚   â”œâ”€â”€ current_market_cap
    â”‚   â”œâ”€â”€ prices (24 hour history)
    â”‚   â”œâ”€â”€ volumes
    â”‚   â””â”€â”€ price_change_24h
    â”‚
    â””â”€â”€ FeatureEngineering.prepare_features()
        â”œâ”€â”€ calculate_technical_indicators()
        â”‚   â”œâ”€â”€ calculate_sma(prices, [7, 14, 30])
        â”‚   â”œâ”€â”€ calculate_ema(prices, [7, 14])
        â”‚   â”œâ”€â”€ calculate_macd(prices)
        â”‚   â”œâ”€â”€ calculate_rsi(prices, 14)
        â”‚   â”œâ”€â”€ calculate_bollinger_bands(prices, 20)
        â”‚   â”œâ”€â”€ calculate_volatility(prices)
        â”‚   â””â”€â”€ calculate_price_position(prices, 14)
        â”‚
        â””â”€â”€ Returns: numpy array (1, 29) with exact features:
            [price, volume, market_cap, sma_7, sma_14, sma_30,
             ema_7, ema_14, macd, macd_signal, macd_histogram,
             rsi, bb_middle, bb_upper, bb_lower, price_change_1h,
             price_change_24h, price_change_7d, volume_sma, volume_ratio,
             volatility, high_14d, low_14d, price_position,
             target_price_24h, target_direction_1h, target_direction_24h,
             target_change_1h, target_change_24h]
    
    Step 5: Model Prediction
    â””â”€â”€ _generate_ensemble_predictions()
        â”œâ”€â”€ Create 3 ensemble variations (noise injection)
        â”‚
        â”œâ”€â”€ Traditional Model Pipeline:
        â”‚   â”œâ”€â”€ UnifiedPredictionPipeline(model_type='traditional')
        â”‚   â”œâ”€â”€ __init__()
        â”‚   â”‚   â”œâ”€â”€ enable_validation=True
        â”‚   â”‚   â”œâ”€â”€ Load: FeatureValidator
        â”‚   â”‚   â””â”€â”€ Load: model.joblib
        â”‚   â””â”€â”€ predict(features)
        â”‚       â”œâ”€â”€ validate_all() (NEW)
        â”‚       â”‚   â”œâ”€â”€ validate_feature_order()
        â”‚       â”‚   â”œâ”€â”€ validate_feature_types()
        â”‚       â”‚   â””â”€â”€ detect_feature_drift()
        â”‚       â””â”€â”€ _predict_traditional()
        â”‚           â””â”€â”€ model.predict(features)
        â”‚
        â”œâ”€â”€ Deep Learning Model Pipeline:
        â”‚   â”œâ”€â”€ UnifiedPredictionPipeline(model_type='deep_learning')
        â”‚   â”œâ”€â”€ __init__()
        â”‚   â”‚   â”œâ”€â”€ enable_validation=True
        â”‚   â”‚   â”œâ”€â”€ Load: FeatureValidator
        â”‚   â”‚   â”œâ”€â”€ Load: model_config.json
        â”‚   â”‚   â”œâ”€â”€ Load: scaler.joblib
        â”‚   â”‚   â””â”€â”€ Load: best_deep_model.pth â†’ CryptoPriceNet
        â”‚   â””â”€â”€ predict(features)
        â”‚       â”œâ”€â”€ validate_all() (NEW)
        â”‚       â””â”€â”€ _predict_deep()
        â”‚           â”œâ”€â”€ scaler.transform(features)
        â”‚           â””â”€â”€ model(features_scaled)
        â”‚
        â””â”€â”€ Ensemble Strategy:
            â”œâ”€â”€ Weight: 70% traditional + 30% deep
            â”‚   (Based on RÂ² scores: 0.998 vs 0.839)
            â”œâ”€â”€ Scale to 5-minute prediction
            â”‚   â”œâ”€â”€ Apply momentum factors (RSI, MACD)
            â”‚   â””â”€â”€ Apply volatility adjustment
            â””â”€â”€ Generate 4 predictions:
                â”œâ”€â”€ +5min, +10min, +15min, +20min
                â””â”€â”€ Each with confidence score
    
    Step 6: Performance Tracking
    â”œâ”€â”€ Read: artifacts/model_performance.jsonl
    â”œâ”€â”€ Verify last prediction (if > 3 minutes old)
    â”‚   â”œâ”€â”€ Calculate error_percent
    â”‚   â””â”€â”€ Update JSONL file
    â””â”€â”€ Store new prediction for future verification
    
    Step 7: Return JSON
    â””â”€â”€ {
          "predictions": [
            {"interval": "+5min", "price": X, "timestamp": "...", "confidence": 0.85},
            {"interval": "+10min", "price": Y, "timestamp": "...", "confidence": 0.82},
            ...
          ],
          "current_price": Z,
          "crypto_id": "bitcoin"
        }
```

---

## ğŸ§© SHARED COMPONENTS (NEW)

### FeatureEngineering Component

**File**: `src/mlProject/components/feature_engineering.py`

**Class**: `FeatureEngineering`
```python
Static Attributes:
â””â”€â”€ FEATURE_NAMES: List[29 feature names in exact order]

Methods:
â”œâ”€â”€ __init__()
â”œâ”€â”€ calculate_sma(prices, period) â†’ float
â”œâ”€â”€ calculate_ema(prices, period) â†’ float
â”œâ”€â”€ calculate_macd(prices) â†’ Dict[macd, signal, histogram]
â”œâ”€â”€ calculate_rsi(prices, period=14) â†’ float
â”œâ”€â”€ calculate_bollinger_bands(prices, period=20) â†’ Dict
â”œâ”€â”€ calculate_volatility(prices) â†’ float
â”œâ”€â”€ calculate_price_position(prices, period=14) â†’ float
â”œâ”€â”€ calculate_technical_indicators(prices, current_price, volumes)
â”‚   â””â”€â”€ Returns: Dict with all 18 indicators
â”œâ”€â”€ prepare_features(current_price, current_volume, current_market_cap, 
â”‚                    prices, volumes, price_change_24h)
â”‚   â”œâ”€â”€ Calls: calculate_technical_indicators()
â”‚   â”œâ”€â”€ Builds: 29-element feature vector
â”‚   â”œâ”€â”€ Validates: Feature count
â”‚   â””â”€â”€ Returns: np.array(1, 29)
â”œâ”€â”€ prepare_features_from_dataframe(df) â†’ DataFrame
â”œâ”€â”€ get_feature_names() â†’ List[str]
â””â”€â”€ validate_features(features) â†’ bool
    â”œâ”€â”€ Checks: 2D array
    â”œâ”€â”€ Checks: 29 features
    â”œâ”€â”€ Checks: No NaN
    â””â”€â”€ Checks: No infinity
```

**Usage**:
- **Training**: Used implicitly (training data already has features)
- **Prediction**: Used explicitly in app.py for live data

---

### FeatureValidator Component

**File**: `src/mlProject/components/feature_validator.py`

**Class**: `FeatureValidator`
```python
Methods:
â”œâ”€â”€ __init__(metadata_path='artifacts/feature_metadata.json')
â”‚   â””â”€â”€ _load_metadata()
â”‚       â””â”€â”€ Loads: feature_names, feature_stats, model_config
â”‚
â”œâ”€â”€ validate_feature_order(feature_names)
â”‚   â”œâ”€â”€ Compares with training feature order
â”‚   â””â”€â”€ Returns: (is_valid, message)
â”‚
â”œâ”€â”€ validate_feature_types(features)
â”‚   â”œâ”€â”€ Checks for NaN values
â”‚   â”œâ”€â”€ Checks for infinite values
â”‚   â””â”€â”€ Returns: (is_valid, message)
â”‚
â”œâ”€â”€ detect_feature_drift(features, threshold=3.0)
â”‚   â”œâ”€â”€ Calculates Z-scores for each feature
â”‚   â”œâ”€â”€ Compares against training statistics
â”‚   â”œâ”€â”€ Detects features outside threshold
â”‚   â””â”€â”€ Returns: (has_drift, drift_report)
â”‚       â””â”€â”€ drift_report: {
â”‚             drifted_features: [],
â”‚             warnings: [],
â”‚             feature_details: {}
â”‚           }
â”‚
â”œâ”€â”€ validate_all(features, feature_names, check_drift, drift_threshold)
â”‚   â”œâ”€â”€ validate_feature_types()
â”‚   â”œâ”€â”€ validate_feature_order()
â”‚   â”œâ”€â”€ detect_feature_drift()
â”‚   â””â”€â”€ Returns: {
â”‚         valid: bool,
â”‚         errors: [],
â”‚         warnings: [],
â”‚         drift_report: {}
â”‚       }
â”‚
â””â”€â”€ Static Methods:
    â”œâ”€â”€ save_feature_metadata(feature_names, feature_stats, 
    â”‚                         model_config, output_path)
    â”‚   â””â”€â”€ Saves JSON with all feature statistics
    â””â”€â”€ calculate_feature_statistics(df)
        â””â”€â”€ Calculates: mean, std, min, max, median, q25, q75
```

**Usage**:
- **Training**: `save_feature_metadata()` called in ModelTrainer
- **Prediction**: `validate_all()` called in UnifiedPredictionPipeline

---

## ğŸ“Š DATA STRUCTURES

### Feature Metadata JSON
```json
{
  "feature_names": ["price", "volume", ...],
  "feature_count": 29,
  "feature_stats": {
    "price": {
      "mean": 45000.5,
      "std": 15000.2,
      "min": 25000.0,
      "max": 70000.0,
      "median": 44000.0,
      "q25": 35000.0,
      "q75": 55000.0
    },
    ...
  },
  "model_config": {
    "model_type": "RandomForest",
    "n_estimators": 100,
    "max_depth": 20,
    "min_samples_split": 5,
    "target_column": "target_price_1h"
  },
  "version": "1.0"
}
```

### Model Config JSON (Deep Learning)
```json
{
  "input_size": 29,
  "hidden_layers": [128, 64, 32],
  "dropout_rate": 0.2,
  "feature_names": [...]
}
```

---

## ğŸ”„ DATA TRANSFORMATIONS

### Column Evolution

**After Data Ingestion** (30 columns):
```
price, volume, market_cap, sma_7, sma_14, sma_30, ema_7, ema_14,
macd, macd_signal, macd_histogram, rsi, bb_middle, bb_upper, bb_lower,
price_change_1h, price_change_24h, price_change_7d, volume_sma, volume_ratio,
volatility, high_14d, low_14d, price_position, target_price_1h, 
target_price_24h, target_direction_1h, target_direction_24h,
target_change_1h, target_change_24h, crypto_symbol
```

**After Data Transformation** (30 columns):
```
[Same as above but crypto_symbol dropped]
```

**During Model Training** (29 features):
```
[All above EXCEPT target_price_1h which is the target variable]
```

**During Prediction** (29 features):
```
[Same 29 features prepared by FeatureEngineering component]
```

---

## ğŸ¯ KEY INSIGHTS

### 1. Feature Consistency
- **Training**: Features calculated during data ingestion
- **Prediction**: Features calculated by FeatureEngineering
- **Validation**: FeatureValidator ensures consistency
- **Result**: Same 29 features used throughout

### 2. Model Architecture
- **Traditional**: Random Forest (RÂ²=0.993, very accurate)
- **Deep Learning**: CryptoPriceNet 3-layer neural network (RÂ²=0.839)
- **Ensemble**: 70% traditional + 30% deep (weighted by performance)

### 3. Data Flow Stages
1. **Ingestion**: 3 APIs â†’ Aggregated CSV
2. **Validation**: Schema checking
3. **Transformation**: Train/test split
4. **Training**: 2 models trained separately
5. **Evaluation**: Metrics logged to MLflow
6. **Prediction**: Live API â†’ Features â†’ Models â†’ Ensemble

### 4. MLOps Best Practices
- âœ… Feature metadata saved during training
- âœ… Feature validation in production
- âœ… Drift detection with Z-scores
- âœ… Configuration-driven architecture
- âœ… MLflow experiment tracking
- âœ… Modular, reusable components

---

## ğŸ“ˆ Performance Metrics

### Training
- **Dataset**: ~1400 samples after cleaning
- **Train/Test Split**: 75% / 25%
- **Traditional Model**: RÂ²=0.998, RMSE~low
- **Deep Learning Model**: RÂ²=0.839, RMSE~moderate

### Prediction
- **API Latency**: ~500ms (with caching)
- **Feature Engineering**: ~5ms
- **Model Inference**: Traditional ~2ms, Deep ~5ms
- **Validation**: ~2ms
- **Total**: < 15ms (excluding API calls)
- **Prediction Error**: ~0.3% average

---

## ğŸ”§ Configuration Files

### config.yaml
- data_ingestion: API URLs, output paths
- data_validation: Schema requirements
- data_transformation: Train/test paths
- model_trainer: Model hyperparameters
- deep_model_trainer: Neural network config
- feature_engineering: Validation settings (NEW)

### params.yaml
- RandomForest: n_estimators=100, max_depth=20, min_samples_split=5
- DeepLearning: hidden_layers, dropout, learning_rate, etc.

### schema.yaml
- COLUMNS: All 30 required column specifications
- TARGET_COLUMN: target_price_1h

---

## ğŸ“ Summary

This project implements a complete MLOps pipeline for cryptocurrency price prediction:

1. **Data Collection**: Multi-source aggregation (CoinGecko, Binance, CryptoCompare)
2. **Feature Engineering**: 29 technical indicators and price targets
3. **Model Training**: Dual approach (Random Forest + PyTorch)
4. **Validation**: Feature validation and drift detection
5. **Deployment**: Flask web application with real-time predictions
6. **Monitoring**: Performance tracking and MLflow logging

The refactored V2.1 architecture ensures feature consistency between training and production through shared components, reducing technical debt and improving maintainability.


{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10e508fb",
   "metadata": {},
   "source": [
    "# LSTM Model Training\n",
    "## Stage 08: Long Short-Term Memory Network for Time Series Prediction\n",
    "\n",
    "This notebook explores training an LSTM model for cryptocurrency price prediction using sequential time series data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e0adfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('../')\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addedc7a",
   "metadata": {},
   "source": [
    "## 1. Configuration Entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33b0065",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class LSTMModelTrainerConfig:\n",
    "    root_dir: Path\n",
    "    train_data_path: Path\n",
    "    test_data_path: Path\n",
    "    model_name: str\n",
    "    hidden_size: int\n",
    "    num_layers: int\n",
    "    dropout_rate: float\n",
    "    learning_rate: float\n",
    "    batch_size: int\n",
    "    epochs: int\n",
    "    early_stopping_patience: int\n",
    "    sequence_length: int\n",
    "    target_column: str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d6dff8",
   "metadata": {},
   "source": [
    "## 2. LSTM Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1216abb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib\n",
    "import json\n",
    "from mlProject import logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3fd4bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CryptoLSTM(nn.Module):\n",
    "    \"\"\"LSTM Network for Cryptocurrency Price Prediction with Time Series\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size=128, num_layers=2, dropout_rate=0.2):\n",
    "        super(CryptoLSTM, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # LSTM layers\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout_rate if num_layers > 1 else 0,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(hidden_size, 64)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch, seq_len, input_size)\n",
    "        \n",
    "        # LSTM forward pass\n",
    "        lstm_out, (h_n, c_n) = self.lstm(x)\n",
    "        \n",
    "        # Use the last hidden state\n",
    "        out = lstm_out[:, -1, :]  # (batch, hidden_size)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        out = self.fc1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc3(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81c93ee",
   "metadata": {},
   "source": [
    "## 3. Time Series Dataset\n",
    "\n",
    "LSTM models require sequential data. We create sequences of historical data to predict future prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242d6c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesDataset(Dataset):\n",
    "    \"\"\"Dataset for creating sequences from time series data\"\"\"\n",
    "    \n",
    "    def __init__(self, features, targets, sequence_length=10):\n",
    "        self.features = features\n",
    "        self.targets = targets\n",
    "        self.sequence_length = sequence_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.features) - self.sequence_length\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Get sequence of features\n",
    "        feature_sequence = self.features[idx:idx + self.sequence_length]\n",
    "        \n",
    "        # Get target (next time step after sequence)\n",
    "        target = self.targets[idx + self.sequence_length]\n",
    "        \n",
    "        return (\n",
    "            torch.FloatTensor(feature_sequence),\n",
    "            torch.FloatTensor([target])\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a12a38d",
   "metadata": {},
   "source": [
    "## 4. LSTM Trainer Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0af806",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModelTrainer:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        logger.info(f\"Using device: {self.device}\")\n",
    "        \n",
    "    def train(self, sequence_length=10):\n",
    "        \"\"\"Train LSTM model with sequential data\"\"\"\n",
    "        try:\n",
    "            # Load data\n",
    "            logger.info(\"Loading training and test data...\")\n",
    "            train_data = pd.read_csv(self.config['train_data_path'])\n",
    "            test_data = pd.read_csv(self.config['test_data_path'])\n",
    "            \n",
    "            # Prepare features and target\n",
    "            target_column = self.config['target_column']\n",
    "            train_x = train_data.drop([target_column], axis=1)\n",
    "            test_x = test_data.drop([target_column], axis=1)\n",
    "            train_y = train_data[target_column].values\n",
    "            test_y = test_data[target_column].values\n",
    "            \n",
    "            input_size = train_x.shape[1]\n",
    "            logger.info(f\"Input features: {input_size}\")\n",
    "            logger.info(f\"Training samples: {len(train_x)}, Test samples: {len(test_x)}\")\n",
    "            logger.info(f\"Sequence length: {sequence_length}\")\n",
    "            \n",
    "            # Scale features\n",
    "            logger.info(\"Scaling features...\")\n",
    "            scaler = StandardScaler()\n",
    "            train_x_scaled = scaler.fit_transform(train_x)\n",
    "            test_x_scaled = scaler.transform(test_x)\n",
    "            \n",
    "            # Save scaler\n",
    "            scaler_path = os.path.join(self.config['root_dir'], 'scaler.joblib')\n",
    "            joblib.dump(scaler, scaler_path)\n",
    "            logger.info(f\"Scaler saved to {scaler_path}\")\n",
    "            \n",
    "            # Create sequences for LSTM\n",
    "            logger.info(f\"Creating sequences with length {sequence_length}...\")\n",
    "            \n",
    "            train_dataset = TimeSeriesDataset(\n",
    "                train_x_scaled, train_y, sequence_length\n",
    "            )\n",
    "            test_dataset = TimeSeriesDataset(\n",
    "                test_x_scaled, test_y, sequence_length\n",
    "            )\n",
    "            \n",
    "            # Data loaders\n",
    "            train_loader = DataLoader(\n",
    "                train_dataset, \n",
    "                batch_size=self.config['batch_size'],\n",
    "                shuffle=True\n",
    "            )\n",
    "            test_loader = DataLoader(\n",
    "                test_dataset,\n",
    "                batch_size=self.config['batch_size'],\n",
    "                shuffle=False\n",
    "            )\n",
    "            \n",
    "            # Initialize LSTM model\n",
    "            model = CryptoLSTM(\n",
    "                input_size=input_size,\n",
    "                hidden_size=self.config['hidden_size'],\n",
    "                num_layers=self.config['num_layers'],\n",
    "                dropout_rate=self.config['dropout_rate']\n",
    "            ).to(self.device)\n",
    "            \n",
    "            logger.info(f\"LSTM Model Architecture:\\n{model}\")\n",
    "            \n",
    "            # Loss and optimizer\n",
    "            criterion = nn.MSELoss()\n",
    "            optimizer = optim.Adam(\n",
    "                model.parameters(), \n",
    "                lr=self.config['learning_rate'],\n",
    "                weight_decay=1e-5\n",
    "            )\n",
    "            scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "                optimizer, mode='min', factor=0.5, patience=5\n",
    "            )\n",
    "            \n",
    "            # Training loop\n",
    "            best_loss = float('inf')\n",
    "            patience_counter = 0\n",
    "            train_losses = []\n",
    "            test_losses = []\n",
    "            \n",
    "            logger.info(\"Starting LSTM training...\")\n",
    "            for epoch in range(self.config['epochs']):\n",
    "                # Training phase\n",
    "                model.train()\n",
    "                train_loss = 0.0\n",
    "                \n",
    "                for batch_features, batch_targets in train_loader:\n",
    "                    batch_features = batch_features.to(self.device)\n",
    "                    batch_targets = batch_targets.to(self.device)\n",
    "                    \n",
    "                    # Forward pass\n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = model(batch_features)\n",
    "                    loss = criterion(outputs, batch_targets)\n",
    "                    \n",
    "                    # Backward pass\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    \n",
    "                    train_loss += loss.item()\n",
    "                \n",
    "                train_loss /= len(train_loader)\n",
    "                train_losses.append(train_loss)\n",
    "                \n",
    "                # Validation phase\n",
    "                model.eval()\n",
    "                test_loss = 0.0\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    for batch_features, batch_targets in test_loader:\n",
    "                        batch_features = batch_features.to(self.device)\n",
    "                        batch_targets = batch_targets.to(self.device)\n",
    "                        \n",
    "                        outputs = model(batch_features)\n",
    "                        loss = criterion(outputs, batch_targets)\n",
    "                        test_loss += loss.item()\n",
    "                \n",
    "                test_loss /= len(test_loader)\n",
    "                test_losses.append(test_loss)\n",
    "                \n",
    "                # Learning rate scheduling\n",
    "                scheduler.step(test_loss)\n",
    "                \n",
    "                # Log progress\n",
    "                if (epoch + 1) % 10 == 0:\n",
    "                    logger.info(\n",
    "                        f\"Epoch [{epoch+1}/{self.config['epochs']}] \"\n",
    "                        f\"Train Loss: {train_loss:.6f}, Test Loss: {test_loss:.6f}\"\n",
    "                    )\n",
    "                \n",
    "                # Early stopping\n",
    "                if test_loss < best_loss:\n",
    "                    best_loss = test_loss\n",
    "                    patience_counter = 0\n",
    "                    \n",
    "                    # Save best model\n",
    "                    model_path = os.path.join(self.config['root_dir'], self.config['model_name'])\n",
    "                    torch.save(model.state_dict(), model_path)\n",
    "                    logger.info(f\"Best LSTM model saved with test loss: {best_loss:.6f}\")\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "                    \n",
    "                if patience_counter >= self.config['early_stopping_patience']:\n",
    "                    logger.info(f\"Early stopping at epoch {epoch+1}\")\n",
    "                    break\n",
    "            \n",
    "            # Save model configuration\n",
    "            model_config = {\n",
    "                'model_type': 'LSTM',\n",
    "                'input_size': input_size,\n",
    "                'hidden_size': self.config['hidden_size'],\n",
    "                'num_layers': self.config['num_layers'],\n",
    "                'dropout_rate': self.config['dropout_rate'],\n",
    "                'sequence_length': sequence_length,\n",
    "                'feature_names': list(train_x.columns)\n",
    "            }\n",
    "            \n",
    "            config_path = os.path.join(self.config['root_dir'], 'model_config.json')\n",
    "            with open(config_path, 'w') as f:\n",
    "                json.dump(model_config, f, indent=4)\n",
    "            logger.info(f\"Model config saved to {config_path}\")\n",
    "            \n",
    "            # Save training history\n",
    "            history = {\n",
    "                'train_losses': train_losses,\n",
    "                'test_losses': test_losses,\n",
    "                'best_loss': best_loss,\n",
    "                'epochs_trained': len(train_losses)\n",
    "            }\n",
    "            \n",
    "            history_path = os.path.join(self.config['root_dir'], 'training_history.json')\n",
    "            with open(history_path, 'w') as f:\n",
    "                json.dump(history, f, indent=4)\n",
    "            logger.info(f\"Training history saved to {history_path}\")\n",
    "            \n",
    "            logger.info(\"LSTM training completed successfully!\")\n",
    "            logger.info(f\"Best test loss: {best_loss:.6f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.exception(f\"Error during LSTM training: {str(e)}\")\n",
    "            raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b48986",
   "metadata": {},
   "source": [
    "## 5. Execute LSTM Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eeb547e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "lstm_config = {\n",
    "    'root_dir': 'artifacts/deep_model_trainer',\n",
    "    'train_data_path': 'artifacts/data_transformation/train.csv',\n",
    "    'test_data_path': 'artifacts/data_transformation/test.csv',\n",
    "    'model_name': 'best_deep_model.pth',\n",
    "    'hidden_size': 128,\n",
    "    'num_layers': 2,\n",
    "    'dropout_rate': 0.2,\n",
    "    'learning_rate': 0.001,\n",
    "    'batch_size': 64,\n",
    "    'epochs': 100,\n",
    "    'early_stopping_patience': 15,\n",
    "    'target_column': 'target_price_1h'\n",
    "}\n",
    "\n",
    "try:\n",
    "    lstm_trainer = LSTMModelTrainer(config=lstm_config)\n",
    "    lstm_trainer.train(sequence_length=10)\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92605ea",
   "metadata": {},
   "source": [
    "## 6. Visualize Training Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22568874",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load training history\n",
    "with open('artifacts/deep_model_trainer/training_history.json', 'r') as f:\n",
    "    history = json.load(f)\n",
    "\n",
    "# Plot losses\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history['train_losses'], label='Train Loss')\n",
    "plt.plot(history['test_losses'], label='Test Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss (MSE)')\n",
    "plt.title('LSTM Training Progress')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history['test_losses'], label='Test Loss', color='orange')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss (MSE)')\n",
    "plt.title('LSTM Test Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nLSTM Training Results:\")\n",
    "print(f\"Best Loss: {history['best_loss']:.6f}\")\n",
    "print(f\"Epochs Trained: {history['epochs_trained']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc051048",
   "metadata": {},
   "source": [
    "## 7. Understanding LSTM Architecture\n",
    "\n",
    "### Why LSTM for Time Series?\n",
    "\n",
    "- **Memory Cells**: LSTMs can remember information for long periods\n",
    "- **Sequential Dependencies**: Captures temporal patterns in crypto prices\n",
    "- **Gates Mechanism**: Input, forget, and output gates control information flow\n",
    "\n",
    "### Our Architecture:\n",
    "1. **Input**: Sequence of 10 time steps with 29 features each\n",
    "2. **LSTM Layers**: 2 layers with 128 hidden units\n",
    "3. **Fully Connected**: 64 → 32 → 1 (price prediction)\n",
    "4. **Regularization**: Dropout (0.2) to prevent overfitting"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

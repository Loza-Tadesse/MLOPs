{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5bae297c",
   "metadata": {},
   "source": [
    "# LSTM Model Evaluation\n",
    "## Stage 09: Evaluating LSTM Performance & MLflow Integration\n",
    "\n",
    "This notebook evaluates the trained LSTM model and compares it with the standard DNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7608376",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('../')\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5769fc7",
   "metadata": {},
   "source": [
    "## 1. Load Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d40e31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import json\n",
    "import joblib\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from mlProject import logger\n",
    "from mlProject.utils.common import save_json\n",
    "from pathlib import Path\n",
    "import mlflow\n",
    "from urllib.parse import urlparse\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934de055",
   "metadata": {},
   "source": [
    "## 2. LSTM Architecture Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e11198",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CryptoLSTM(nn.Module):\n",
    "    \"\"\"LSTM Network for Cryptocurrency Price Prediction\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size=128, num_layers=2, dropout_rate=0.2):\n",
    "        super(CryptoLSTM, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # LSTM layers\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout_rate if num_layers > 1 else 0,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(hidden_size, 64)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        lstm_out, (h_n, c_n) = self.lstm(x)\n",
    "        out = lstm_out[:, -1, :]\n",
    "        out = self.fc1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc3(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16bd918a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    \"\"\"Dataset for LSTM sequences\"\"\"\n",
    "    \n",
    "    def __init__(self, features, targets, sequence_length=10):\n",
    "        self.features = features\n",
    "        self.targets = targets\n",
    "        self.sequence_length = sequence_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.features) - self.sequence_length\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        feature_sequence = self.features[idx:idx + self.sequence_length]\n",
    "        target = self.targets[idx + self.sequence_length]\n",
    "        return (\n",
    "            torch.FloatTensor(feature_sequence),\n",
    "            torch.FloatTensor([target])\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b172e938",
   "metadata": {},
   "source": [
    "## 3. LSTM Evaluation Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddaab42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModelEvaluation:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    def eval_metrics(self, actual, pred):\n",
    "        rmse = np.sqrt(mean_squared_error(actual, pred))\n",
    "        mae = mean_absolute_error(actual, pred)\n",
    "        r2 = r2_score(actual, pred)\n",
    "        return rmse, mae, r2\n",
    "\n",
    "    def log_into_mlflow(self):\n",
    "        try:\n",
    "            # Load test data\n",
    "            test_data = pd.read_csv(self.config['test_data_path'])\n",
    "            \n",
    "            # Load model configuration\n",
    "            with open(self.config['model_config_path'], 'r') as f:\n",
    "                model_config = json.load(f)\n",
    "            \n",
    "            # Load scaler\n",
    "            scaler = joblib.load(self.config['scaler_path'])\n",
    "            \n",
    "            # Prepare test data\n",
    "            test_x = test_data.drop([self.config['target_column']], axis=1)\n",
    "            test_y = test_data[self.config['target_column']].values\n",
    "            \n",
    "            # Scale features\n",
    "            test_x_scaled = scaler.transform(test_x)\n",
    "            \n",
    "            # Get sequence length from model config\n",
    "            sequence_length = model_config.get('sequence_length', 10)\n",
    "            \n",
    "            # Create dataset\n",
    "            test_dataset = TimeSeriesDataset(test_x_scaled, test_y, sequence_length)\n",
    "            \n",
    "            # Initialize LSTM model\n",
    "            model = CryptoLSTM(\n",
    "                input_size=model_config['input_size'],\n",
    "                hidden_size=model_config['hidden_size'],\n",
    "                num_layers=model_config['num_layers'],\n",
    "                dropout_rate=model_config['dropout_rate']\n",
    "            ).to(self.device)\n",
    "            \n",
    "            # Load trained weights\n",
    "            model.load_state_dict(torch.load(self.config['model_path'], map_location=self.device))\n",
    "            model.eval()\n",
    "            \n",
    "            # Make predictions\n",
    "            predictions = []\n",
    "            actuals = []\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for i in range(len(test_dataset)):\n",
    "                    features, target = test_dataset[i]\n",
    "                    features = features.unsqueeze(0).to(self.device)\n",
    "                    pred = model(features).cpu().numpy().flatten()[0]\n",
    "                    predictions.append(pred)\n",
    "                    actuals.append(target.item())\n",
    "            \n",
    "            predictions = np.array(predictions)\n",
    "            actuals = np.array(actuals)\n",
    "            \n",
    "            # Calculate metrics\n",
    "            (rmse, mae, r2) = self.eval_metrics(actuals, predictions)\n",
    "            \n",
    "            # Save metrics locally\n",
    "            scores = {\"rmse\": rmse, \"mae\": mae, \"r2\": r2}\n",
    "            save_json(path=Path(self.config['metric_file_name']), data=scores)\n",
    "            \n",
    "            # MLflow logging\n",
    "            mlflow.set_registry_uri(self.config['mlflow_uri'])\n",
    "            \n",
    "            # Set experiment\n",
    "            experiment_name = \"LSTM_CryptoPredict\"\n",
    "            try:\n",
    "                mlflow.create_experiment(experiment_name)\n",
    "            except mlflow.exceptions.MlflowException:\n",
    "                pass\n",
    "            \n",
    "            mlflow.set_experiment(experiment_name)\n",
    "            \n",
    "            # Create unique run name\n",
    "            import time\n",
    "            run_name = f\"lstm_eval_{int(time.time())}\"\n",
    "            \n",
    "            with mlflow.start_run(run_name=run_name):\n",
    "                # Log parameters\n",
    "                params_to_log = {\n",
    "                    \"model_type\": \"LSTM\",\n",
    "                    \"hidden_size\": model_config['hidden_size'],\n",
    "                    \"num_layers\": model_config['num_layers'],\n",
    "                    \"sequence_length\": sequence_length,\n",
    "                    \"input_size\": model_config['input_size'],\n",
    "                    \"dropout_rate\": model_config['dropout_rate'],\n",
    "                    \"device\": str(self.device)\n",
    "                }\n",
    "                \n",
    "                for key, value in params_to_log.items():\n",
    "                    try:\n",
    "                        mlflow.log_param(key, value)\n",
    "                    except mlflow.exceptions.MlflowException as param_e:\n",
    "                        logger.warning(f\"Could not log parameter {key}: {param_e}\")\n",
    "                \n",
    "                # Log metrics\n",
    "                mlflow.log_metric(\"rmse\", rmse)\n",
    "                mlflow.log_metric(\"mae\", mae)\n",
    "                mlflow.log_metric(\"r2\", r2)\n",
    "                \n",
    "                # Log model\n",
    "                tracking_url_type_store = urlparse(mlflow.get_tracking_uri()).scheme\n",
    "                if tracking_url_type_store != \"file\":\n",
    "                    mlflow.pytorch.log_model(model, \"model\", registered_model_name=\"LSTMCryptoPriceModel\")\n",
    "                else:\n",
    "                    mlflow.pytorch.log_model(model, \"model\")\n",
    "            \n",
    "            logger.info(f\"LSTM evaluation completed. RMSE: {rmse:.4f}, MAE: {mae:.4f}, R2: {r2:.4f}\")\n",
    "            return scores, predictions, actuals\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.exception(f\"Error during LSTM evaluation: {str(e)}\")\n",
    "            raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c633816e",
   "metadata": {},
   "source": [
    "## 4. Execute LSTM Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a005af41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "evaluation_config = {\n",
    "    'test_data_path': 'artifacts/data_transformation/test.csv',\n",
    "    'model_path': 'artifacts/deep_model_trainer/best_deep_model.pth',\n",
    "    'scaler_path': 'artifacts/deep_model_trainer/scaler.joblib',\n",
    "    'model_config_path': 'artifacts/deep_model_trainer/model_config.json',\n",
    "    'metric_file_name': 'artifacts/deep_model_evaluation/metrics.json',\n",
    "    'target_column': 'target_price_1h',\n",
    "    'mlflow_uri': 'https://dagshub.com/Loza-Tadesse/SolPredict-AI.mlflow'\n",
    "}\n",
    "\n",
    "try:\n",
    "    lstm_evaluation = LSTMModelEvaluation(config=evaluation_config)\n",
    "    scores, predictions, actuals = lstm_evaluation.log_into_mlflow()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"LSTM Model Performance\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"RMSE: {scores['rmse']:.4f}\")\n",
    "    print(f\"MAE:  {scores['mae']:.4f}\")\n",
    "    print(f\"R²:   {scores['r2']:.4f}\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79113caa",
   "metadata": {},
   "source": [
    "## 5. Visualize LSTM Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f63f3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predictions vs actual\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Scatter plot\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.scatter(actuals, predictions, alpha=0.5)\n",
    "plt.plot([actuals.min(), actuals.max()], [actuals.min(), actuals.max()], 'r--', lw=2)\n",
    "plt.xlabel('Actual Price')\n",
    "plt.ylabel('Predicted Price')\n",
    "plt.title('LSTM: Actual vs Predicted Prices')\n",
    "plt.grid(True)\n",
    "\n",
    "# Time series plot (first 200 points)\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(actuals[:200], label='Actual', alpha=0.7)\n",
    "plt.plot(predictions[:200], label='Predicted', alpha=0.7)\n",
    "plt.xlabel('Sample Index')\n",
    "plt.ylabel('Price')\n",
    "plt.title('LSTM: Time Series Prediction (First 200 samples)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Residuals\n",
    "plt.subplot(2, 2, 3)\n",
    "residuals = actuals - predictions\n",
    "plt.hist(residuals, bins=50, edgecolor='black')\n",
    "plt.xlabel('Residuals (Actual - Predicted)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('LSTM: Residuals Distribution')\n",
    "plt.grid(True)\n",
    "\n",
    "# Residuals over time\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.scatter(range(len(residuals)), residuals, alpha=0.5)\n",
    "plt.axhline(y=0, color='r', linestyle='--')\n",
    "plt.xlabel('Sample Index')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('LSTM: Residuals Over Time')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print statistical summary\n",
    "print(\"\\nResiduals Statistics:\")\n",
    "print(f\"Mean: {residuals.mean():.4f}\")\n",
    "print(f\"Std:  {residuals.std():.4f}\")\n",
    "print(f\"Min:  {residuals.min():.4f}\")\n",
    "print(f\"Max:  {residuals.max():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ae3b6c",
   "metadata": {},
   "source": [
    "## 6. Compare LSTM vs DNN Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed818b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load both model metrics\n",
    "with open('artifacts/deep_model_evaluation/metrics.json', 'r') as f:\n",
    "    metrics = json.load(f)\n",
    "\n",
    "# Create comparison\n",
    "comparison_data = {\n",
    "    'Model': ['LSTM', 'Standard DNN'],\n",
    "    'RMSE': [scores['rmse'], metrics.get('rmse', 'N/A')],\n",
    "    'MAE': [scores['mae'], metrics.get('mae', 'N/A')],\n",
    "    'R²': [scores['r2'], metrics.get('r2', 'N/A')]\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Model Performance Comparison\")\n",
    "print(\"=\"*60)\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Visualize comparison\n",
    "if isinstance(comparison_df['RMSE'][1], (int, float)):\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    metrics_to_plot = ['RMSE', 'MAE', 'R²']\n",
    "    colors = ['#FF6B6B', '#4ECDC4']\n",
    "    \n",
    "    for idx, metric in enumerate(metrics_to_plot):\n",
    "        axes[idx].bar(comparison_df['Model'], comparison_df[metric], color=colors)\n",
    "        axes[idx].set_ylabel(metric)\n",
    "        axes[idx].set_title(f'{metric} Comparison')\n",
    "        axes[idx].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc2e271",
   "metadata": {},
   "source": [
    "## 7. Key Insights\n",
    "\n",
    "### LSTM Advantages:\n",
    "- **Temporal Memory**: Captures long-term dependencies in price movements\n",
    "- **Sequential Learning**: Learns from historical patterns\n",
    "- **Better for volatile markets**: Handles sudden price changes\n",
    "\n",
    "### When to use LSTM vs DNN:\n",
    "- **LSTM**: When temporal patterns are critical (hourly/daily predictions)\n",
    "- **DNN**: When feature relationships matter more than sequence\n",
    "- **Ensemble**: Combine both for robust predictions\n",
    "\n",
    "### Model Selection Criteria:\n",
    "1. Lower RMSE = Better price accuracy\n",
    "2. Higher R² = Better variance explanation\n",
    "3. Lower MAE = Better average error"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
